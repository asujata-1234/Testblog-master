{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Utilities.plot import histvstarget, distribution\n",
    "from IPython.core.display import display, HTML\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "from datetime import datetime, date\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import scipy.stats as st\n",
    "import scipy.io as spio\n",
    "import sklearn.preprocessing as skpp\n",
    "import scipy.sparse.linalg as ll\n",
    "from skimage.measure import block_reduce\n",
    "from os.path import relpath, exists\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from scipy.spatial.distance import cdist \n",
    "import networkx as nx\n",
    "#from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "import sklearn.utils.graph_shortest_path as gspath\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import plotly.express as px\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "import pydotplus\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pydotplus\n",
    "from IPython.display import Image  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from the data exploration notebook that we now have three different data sets now, the original, one with PCA and one with attribute selection using BFE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets take 10% of the data out to be used as a validation set and create the three different training-sets from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('input/Train.csv')\n",
    "cols = train_data.columns\n",
    "#train_data = train_data.dropna(axis=0)\n",
    "train_data = train_data.fillna(train_data.mean())\n",
    "train_data\n",
    "Y = train_data['Outcome']\n",
    "X = train_data.drop(['Outcome'], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x_train = np.mean(x_train)\n",
    "std_x_train = np.std(x_train)\n",
    "std = lambda x: ((x - mean_x_train)/std_x_train)\n",
    "inv_std = lambda x: ((x*std_x_train + mean_x_train))\n",
    "x_train_std = std(x_train)\n",
    "x_test_std = std(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.193734</td>\n",
       "      <td>0.651919</td>\n",
       "      <td>-0.573318</td>\n",
       "      <td>-0.029343</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>0.308324</td>\n",
       "      <td>-1.063375</td>\n",
       "      <td>0.630527</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.845116</td>\n",
       "      <td>1.456855</td>\n",
       "      <td>1.233004</td>\n",
       "      <td>-0.049550</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>0.322695</td>\n",
       "      <td>1.181396</td>\n",
       "      <td>1.562256</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.845116</td>\n",
       "      <td>-0.474992</td>\n",
       "      <td>-1.065951</td>\n",
       "      <td>1.895660</td>\n",
       "      <td>0.268393</td>\n",
       "      <td>0.394554</td>\n",
       "      <td>-0.236049</td>\n",
       "      <td>-0.809417</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.845116</td>\n",
       "      <td>-0.120820</td>\n",
       "      <td>-1.558585</td>\n",
       "      <td>-1.880335</td>\n",
       "      <td>-1.232557</td>\n",
       "      <td>-1.502511</td>\n",
       "      <td>-0.843526</td>\n",
       "      <td>-0.809417</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.262588</td>\n",
       "      <td>1.617842</td>\n",
       "      <td>0.411948</td>\n",
       "      <td>1.094691</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.150235</td>\n",
       "      <td>1.369425</td>\n",
       "      <td>-0.216498</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0     1.193734  0.651919      -0.573318      -0.029343  0.021280  0.308324   \n",
       "1    -0.845116  1.456855       1.233004      -0.049550  0.021280  0.322695   \n",
       "2    -0.845116 -0.474992      -1.065951       1.895660  0.268393  0.394554   \n",
       "3    -0.845116 -0.120820      -1.558585      -1.880335 -1.232557 -1.502511   \n",
       "4    -0.262588  1.617842       0.411948       1.094691  0.350477  0.150235   \n",
       "\n",
       "   DiabetesPedigreeFunction       Age  Outcome  \n",
       "0                 -1.063375  0.630527      1.0  \n",
       "1                  1.181396  1.562256      1.0  \n",
       "2                 -0.236049 -0.809417      0.0  \n",
       "3                 -0.843526 -0.809417      0.0  \n",
       "4                  1.369425 -0.216498      1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingdata = pd.DataFrame(np.hstack((x_train_std, y_train[:,np.newaxis])), columns = cols)\n",
    "validationdata = pd.DataFrame(np.hstack((x_test_std, y_test[:,np.newaxis])), columns = cols)\n",
    "trainingdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(518, 5)\n"
     ]
    }
   ],
   "source": [
    "# Dataset with PCA\n",
    "import Utilities.mypca as PCA\n",
    "pca = PCA.MyPCA()\n",
    "pca.fit(x_train)\n",
    "PCA_x_train = pca.fit_transform(5, x_train)\n",
    "PCA_x_val = pca.fit_transform(5, x_test)\n",
    "print(PCA_x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(518, 8)\n"
     ]
    }
   ],
   "source": [
    "x = trainingdata[[\"Glucose\", \"Pregnancies\", \"BMI\", \"SkinThickness\", \"Insulin\", \"BloodPressure\",\"DiabetesPedigreeFunction\",\"Age\"]]\n",
    "y = trainingdata[['Outcome']]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lm = LinearRegression()\n",
    "model_1=lm.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>0.022368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>0.006319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>-0.002885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>-0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>-0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>0.015765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>0.163283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.000731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Coefficient\n",
       "Glucose                      0.022368\n",
       "Pregnancies                  0.006319\n",
       "BMI                         -0.002885\n",
       "SkinThickness               -0.000188\n",
       "Insulin                     -0.000244\n",
       "BloodPressure                0.015765\n",
       "DiabetesPedigreeFunction     0.163283\n",
       "Age                          0.000731"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_1.coef_.T,x.columns,columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Outcome'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_1 = model_1.predict(x_test)\n",
    "sns.regplot(y_test,predictions_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30768985461980203"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                Outcome   R-squared:                       0.308\n",
      "Model:                            OLS   Adj. R-squared:                  0.297\n",
      "Method:                 Least Squares   F-statistic:                     28.28\n",
      "Date:                Fri, 03 Dec 2021   Prob (F-statistic):           1.91e-36\n",
      "Time:                        02:13:11   Log-Likelihood:                -258.04\n",
      "No. Observations:                 518   AIC:                             534.1\n",
      "Df Residuals:                     509   BIC:                             572.3\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "============================================================================================\n",
      "                               coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "const                       -0.8757      0.129     -6.798      0.000      -1.129      -0.623\n",
      "Pregnancies                  0.0224      0.006      3.618      0.000       0.010       0.035\n",
      "Glucose                      0.0063      0.001      9.579      0.000       0.005       0.008\n",
      "BloodPressure               -0.0029      0.002     -1.784      0.075      -0.006       0.000\n",
      "SkinThickness               -0.0002      0.002     -0.075      0.940      -0.005       0.005\n",
      "Insulin                     -0.0002      0.000     -1.062      0.289      -0.001       0.000\n",
      "BMI                          0.0158      0.003      4.824      0.000       0.009       0.022\n",
      "DiabetesPedigreeFunction     0.1633      0.052      3.121      0.002       0.060       0.266\n",
      "Age                          0.0007      0.002      0.379      0.705      -0.003       0.005\n",
      "==============================================================================\n",
      "Omnibus:                       31.768   Durbin-Watson:                   1.985\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               19.686\n",
      "Skew:                           0.339   Prob(JB):                     5.31e-05\n",
      "Kurtosis:                       2.328   Cond. No.                     1.68e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.68e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "x_train_sm = sm.add_constant(x_train)\n",
    "ls = sm.OLS(y_train,x_train_sm).fit()\n",
    "print(ls.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.15\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error: %.2f\" % np.mean((model_1.predict(x_test) - y_test) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_logistic = LogisticRegression(random_state=0,max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = lm_logistic.fit(x_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200, random_state=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.791907514450867"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logistic.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******** For i = 0.05 ******\n",
      "Our testing accuracy is 0.4277456647398844\n",
      "\n",
      "******** For i = 0.1 ******\n",
      "Our testing accuracy is 0.5838150289017341\n",
      "\n",
      "******** For i = 0.15 ******\n",
      "Our testing accuracy is 0.6589595375722543\n",
      "\n",
      "******** For i = 0.2 ******\n",
      "Our testing accuracy is 0.6994219653179191\n",
      "\n",
      "******** For i = 0.25 ******\n",
      "Our testing accuracy is 0.7052023121387283\n",
      "\n",
      "******** For i = 0.3 ******\n",
      "Our testing accuracy is 0.7630057803468208\n",
      "\n",
      "******** For i = 0.35 ******\n",
      "Our testing accuracy is 0.7630057803468208\n",
      "\n",
      "******** For i = 0.4 ******\n",
      "Our testing accuracy is 0.7630057803468208\n",
      "\n",
      "******** For i = 0.45 ******\n",
      "Our testing accuracy is 0.791907514450867\n",
      "\n",
      "******** For i = 0.5 ******\n",
      "Our testing accuracy is 0.791907514450867\n",
      "\n",
      "******** For i = 0.55 ******\n",
      "Our testing accuracy is 0.791907514450867\n",
      "\n",
      "******** For i = 0.6 ******\n",
      "Our testing accuracy is 0.7861271676300579\n",
      "\n",
      "******** For i = 0.65 ******\n",
      "Our testing accuracy is 0.7687861271676301\n",
      "\n",
      "******** For i = 0.7 ******\n",
      "Our testing accuracy is 0.7398843930635838\n",
      "\n",
      "******** For i = 0.75 ******\n",
      "Our testing accuracy is 0.7341040462427746\n",
      "\n",
      "******** For i = 0.8 ******\n",
      "Our testing accuracy is 0.7052023121387283\n",
      "\n",
      "******** For i = 0.85 ******\n",
      "Our testing accuracy is 0.6705202312138728\n",
      "\n",
      "******** For i = 0.9 ******\n",
      "Our testing accuracy is 0.6763005780346821\n",
      "\n",
      "******** For i = 0.95 ******\n",
      "Our testing accuracy is 0.6589595375722543\n",
      "\n",
      "******** For i = 0.99 ******\n",
      "Our testing accuracy is 0.6589595375722543\n"
     ]
    }
   ],
   "source": [
    "pred_proba_df = pd.DataFrame(model_logistic.predict_proba(x_test))\n",
    "threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]\n",
    "for i in threshold_list:\n",
    "    print ('\\n******** For i = {} ******'.format(i))\n",
    "    y_test_pred = np.array(pred_proba_df.iloc[:,1].apply(lambda x: 1 if x>i else 0))\n",
    "    test_accuracy = metrics.accuracy_score(np.array(y_test), y_test_pred)\n",
    "    print('Our testing accuracy is {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date\n",
    "import math\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from skmultilearn.adapt import MLkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "518/518 [==============================] - 0s 446us/step - loss: 16.7204 - accuracy: 0.4247\n",
      "Epoch 2/10\n",
      "518/518 [==============================] - 0s 113us/step - loss: 2.3732 - accuracy: 0.4691\n",
      "Epoch 3/10\n",
      "518/518 [==============================] - 0s 103us/step - loss: 1.7489 - accuracy: 0.4942\n",
      "Epoch 4/10\n",
      "518/518 [==============================] - 0s 99us/step - loss: 1.4581 - accuracy: 0.4826\n",
      "Epoch 5/10\n",
      "518/518 [==============================] - 0s 96us/step - loss: 1.1089 - accuracy: 0.5039\n",
      "Epoch 6/10\n",
      "518/518 [==============================] - 0s 97us/step - loss: 1.0286 - accuracy: 0.4981\n",
      "Epoch 7/10\n",
      "518/518 [==============================] - 0s 107us/step - loss: 0.9449 - accuracy: 0.5483\n",
      "Epoch 8/10\n",
      "518/518 [==============================] - 0s 103us/step - loss: 0.8559 - accuracy: 0.5695\n",
      "Epoch 9/10\n",
      "518/518 [==============================] - 0s 103us/step - loss: 0.8328 - accuracy: 0.6158\n",
      "Epoch 10/10\n",
      "518/518 [==============================] - 0s 97us/step - loss: 0.7758 - accuracy: 0.6042\n",
      "173/173 [==============================] - 0s 158us/step\n",
      "Accuracy: 62.43\n"
     ]
    }
   ],
   "source": [
    "#Deep learning model\n",
    "\n",
    "model_11 = Sequential()\n",
    "model_11.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model_11.add(Dense(10, activation='relu'))\n",
    "model_11.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_11.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model_11.fit(x_train, y_train, epochs=10, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model_11.evaluate(x_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4913294797687861"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy Prediction\n",
    "model_11.predict(x_test)\n",
    "prediction_11= model_11.predict(x_test)\n",
    "prediction_11[prediction_11 < 0.1] = 0\n",
    "prediction_11[prediction_11 >= 0.1] = 1\n",
    "\n",
    "metrics.accuracy_score(prediction_11,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test,prediction_11) \n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.imshow(cm)\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models with K-fold cross validation using accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is small the best way to approach it is probably by using shallow learners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets establish a baseline for the problem by using cross validation and a few classification models. If we use k-fold cross validation since our dataset is quite small a splitting of our data into to few folds could introduce a substantial bias. On the other hand if we chose k to large we will have a lot of variance. With the small data-set in the back of our head we chose k to be moderately large, 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "GB = GaussianNB() # Gaussian NB since we have continous data.\n",
    "KN = KNeighborsClassifier(n_neighbors=4, p = 2) # 4 Neighbors by euclidian distance.\n",
    "DT_GINI = DecisionTreeClassifier(criterion=\"gini\",max_depth=4) # Decision Tree with Gini impurity for quality of split\n",
    "DT_IG = DecisionTreeClassifier(criterion=\"entropy\",max_depth=4) # Information gain for quality of split\n",
    "SV = SVC() # Support vector machine classifier.\n",
    "\n",
    "modelnames = [\"Logistic regression\",\"Gaussian Naive Bayes\", \"4-Neighbors\",\"Decisiontree Gini\",\n",
    "              \"Decisiontree Information gain\", \"Support vector machine\"]\n",
    "\n",
    "models = zip(modelnames,[LR, GB, KN, DT_GINI, DT_IG, SV])\n",
    "results_PCA = []\n",
    "results_BFE = []\n",
    "results = []\n",
    "\n",
    "for name,model in models:\n",
    "    kfold = KFold(n_splits=18)\n",
    "    cv_result_PCA = cross_val_score(model, PCA_x_train, y_train, cv = kfold,scoring = \"accuracy\")\n",
    "    cv_result_BFE = cross_val_score(model, BFE_x_train, y_train, cv = kfold,scoring = \"accuracy\")\n",
    "    cv_result = cross_val_score(model, x_train, y_train, cv = kfold,scoring = \"accuracy\")\n",
    "    results_PCA.append(cv_result_PCA)\n",
    "    results_BFE.append(cv_result_BFE)\n",
    "    results.append(cv_result)\n",
    "\n",
    "print(\"PCA\")\n",
    "for name, res in zip(modelnames, results_PCA):\n",
    "    print(name,res.mean())\n",
    "print()\n",
    "print(\"BFE\")\n",
    "for name, res in zip(modelnames, results_BFE):\n",
    "    print(name,res.mean())\n",
    "print()\n",
    "print(\"All Data\")\n",
    "for name, res in zip(modelnames, results):\n",
    "    print(name,res.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15,10))\n",
    "plt.subplot(3, 1, 1)\n",
    "ax = sns.boxplot(data=results_PCA)\n",
    "ax.set_title(\"PCA\")\n",
    "ax.set_xticklabels(modelnames)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "ax = sns.boxplot(data=results_BFE)\n",
    "ax.set_title(\"BFE\")\n",
    "ax.set_xticklabels(modelnames)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "ax = sns.boxplot(data=results)\n",
    "ax.set_xticklabels(modelnames)\n",
    "ax.set_title(\"Raw data\")\n",
    "#plt.savefig('modelsvsdata.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no sign of significantly better accuracy for any of the data sets, different models perform differently across the datasets. Logistic regression seems to be the best performing model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrixes and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since k-fold-cross validation did not give us any clear information, lets have a look at the confusion matrixes and accuracy on the validation data for the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "models = zip(modelnames,[LR, GB, KN, DT_GINI, DT_IG, SV])\n",
    "\n",
    "i=1\n",
    "xlabel = [\"Predicted 0\",\"Predicted 1\"]\n",
    "ylabel = [\"True 0\",\"True 1\"]\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(PCA_x_train, y_train)\n",
    "    predictions=model.predict(PCA_x_val)\n",
    "    conf_PCA = (confusion_matrix(y_val,predictions))\n",
    "    plt.subplot(3, 6, i)\n",
    "    sns.heatmap(conf_PCA, annot=True, xticklabels=xlabel, yticklabels=ylabel)    \n",
    "    acc = (accuracy_score(y_val,predictions))\n",
    "    plt.title(name + \"\\n PCA \" + '{0:.{1}f}'.format(acc, 3))\n",
    "    model.fit(BFE_x_train, y_train)\n",
    "    predictions=model.predict(BFE_x_val)\n",
    "    conf_BFE = (confusion_matrix(y_val,predictions))\n",
    "    plt.subplot(3, 6, i+6)\n",
    "    sns.heatmap(conf_BFE, annot=True, xticklabels=xlabel, yticklabels=ylabel)  \n",
    "    acc = (accuracy_score(y_val,predictions))\n",
    "    plt.title(\"BFE \" + '{0:.{1}f}'.format(acc, 3))\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions=model.predict(x_val)\n",
    "    conf = (confusion_matrix(y_val,predictions))\n",
    "    plt.subplot(3, 6, i+12)\n",
    "    sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)  \n",
    "    acc = (accuracy_score(y_val,predictions))\n",
    "    plt.title(\"All Data \" + '{0:.{1}f}'.format(acc, 3))\n",
    "    i = i + 1\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('confusion.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that, apparently almost all models predict the unseen validation badly. The model that classifies the validation set significantly better is the decision trees. As we inspect the confusion matrix we note that most of the models have almost identical predictions when it comes to instances with true negative outcomes. Where they differ and where some of the the decision trees is superior is when we try to predict true positive outcomes. Many of the models are worse than random guessing. So why is this? It's called the False positive paradox. Basically since the majority of the instances has outcome not diabetes our models will favor predicting not diabetes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training instances with outcome diabetes\", np.count_nonzero(y_train))\n",
    "print(\"Number of training instances with outcome not diabetes\", len(y_train)-np.count_nonzero(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to another paradox, namely the accuracy paradox. Basically it means that because of the inbalance in outcomes predictive models with a given certain accuracy might have greater predictive capability than a model with higher accuracy. For example, a model with a 1:10 ratio between positive and negative outcomes, say 15 and 150. Predicting everything as negative gives us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_train = [1 if i%10==0 else 0 for i in range(100)]\n",
    "prediction = [0 for i in range(len(example_train))]\n",
    "print(\"An accuracy when 10:1 ratio of\", accuracy_score(example_train,prediction))\n",
    "\n",
    "example_train = [1 if i < 102 else 0 for i in range(102+213)]\n",
    "prediction = [0 for i in range(len(example_train))]\n",
    "print(\"An accuracy when 2:1 ratio (our case) of\", accuracy_score(example_train,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our completely useless models with zero predictive power have 90% respectively 68% accuracy. In our data the inbalance is not as sever but the moral remains. Hence we exchange accuracy as metric in favor of AUC - The area under the ROC (receiver operating characteristic) curve. The ROC curve is the true positive rate (TPR) against the false positive rate (FPR) at various thresholds/ranks for the instances. The area under the curve measures discrimination, the ability to correctly classify those with and without diabetes in our case. An simple interpretation is the following: consider if we randomly draw one person that has diabetes and one without, the person with high ranking (or low) should be the one with diabetes. The area under the curve is the percentage of randomly drawn pairs for which this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"PCA AUC\")\n",
    "for name, model in zip(modelnames,[LR, GB, KN, DT_GINI, DT_IG, SV]):\n",
    "    model.fit(PCA_x_train, y_train)\n",
    "    predictions=model.predict(PCA_x_val)\n",
    "    roc = (roc_auc_score(y_val,predictions))\n",
    "    print(name, roc)\n",
    "    \n",
    "print(\"\\nBFE AUC\")\n",
    "for name, model in zip(modelnames,[LR, GB, KN, DT_GINI, DT_IG, SV]):\n",
    "    model.fit(BFE_x_train, y_train)\n",
    "    predictions=model.predict(BFE_x_val)\n",
    "    roc = (roc_auc_score(y_val,predictions))\n",
    "    print(name, roc)\n",
    "\n",
    "print(\"\\nAll data AUC\")\n",
    "for name, model in zip(modelnames,[LR, GB, KN, DT_GINI, DT_IG, SV]):\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions=model.predict(x_val)\n",
    "    roc = (roc_auc_score(y_val,predictions))\n",
    "    print(name, roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Inspecting the confusion matrixes and AUC scores we note that the absolutely best model is the decision tree with gini impurity or entropy and using all the data, hence that is the model we'll move on with. This is good in the sense that interpretability is available, somethinng that might be important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As familiar decision trees have high variance. As such we now look at the effect of increasing the maximum depth of the tree and at the same time inspecting the deviation of 100 trees at that depth. The maximum leaf nodes is fixed to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def exploredepth(cr, max_leaf_nodes):\n",
    "    depth = np.arange(1,10,1)\n",
    "    t = []\n",
    "    v = []\n",
    "    stdt = []\n",
    "    stdv = []\n",
    "    conf = []\n",
    "    falpos = []\n",
    "    falneg = []\n",
    "    for i in depth:\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        conf = []\n",
    "        conf2 = []\n",
    "        for j in range(1,100):\n",
    "            model = DecisionTreeClassifier(criterion=cr, splitter='best', \n",
    "                                       max_depth=i, min_samples_split=2, min_samples_leaf=1, \n",
    "                                       min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "                                       max_leaf_nodes=max_leaf_nodes, min_impurity_split=1e-07, \n",
    "                                           class_weight=None)\n",
    "            model.fit(x_train, y_train)\n",
    "            val_loss.append((roc_auc_score(y_val,model.predict(x_val))))\n",
    "            train_loss.append((roc_auc_score(y_train,model.predict(x_train))))\n",
    "            c = confusion_matrix(y_val,model.predict(x_val))\n",
    "            conf.append(c[1][0])\n",
    "            conf2.append(c[0][1])\n",
    "        falpos.append(np.mean(conf))\n",
    "        falneg.append(np.mean(conf2))\n",
    "        t.append(np.mean(train_loss))\n",
    "        stdt.append(np.std(train_loss))\n",
    "        v.append(np.mean(val_loss))\n",
    "        stdv.append(np.std(val_loss))\n",
    "    lw = 2\n",
    "    plt.plot(depth,t,color=\"darkorange\")\n",
    "    tp =plt.fill_between(depth, np.array(t) - np.array(stdt),\n",
    "                     np.array(t) + np.array(stdt), alpha=0.2,\n",
    "                     color=\"darkorange\", lw=lw)\n",
    "    plt.plot(depth,v,color = \"navy\")\n",
    "    vp = plt.fill_between(depth, np.array(v) - np.array(stdv),\n",
    "                     np.array(v) + np.array(stdv), alpha=0.2,\n",
    "                     color=\"navy\", lw=lw)\n",
    "    fp = plt.scatter(depth, np.array(falpos)/10.0, color=\"red\")\n",
    "    fn = plt.scatter(depth, np.array(falneg)/10.0, color=\"green\")\n",
    "    plt.legend((tp,vp,fp,fn),\n",
    "               ['validation AUC','training AUC','Number of false negatives (1e-1)','Number of false positives (1e-1)']\n",
    "               , loc='lower left')\n",
    "    plt.xlabel('Depth')\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('entropy')\n",
    "exploredepth('entropy', 25)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('gini')\n",
    "exploredepth('gini', 25)\n",
    "#plt.savefig('xploredepth.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot does not imply any overfitting, by inspection the best model is the decision tree with entropy criterion using a leaf node bound of 25, and depth bound of 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a more rigorus parameter search we're gonna search an selected subset of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import zero_one_loss, f1_score\n",
    "from Utilities.utils import gridsearch\n",
    "\n",
    "model = DecisionTreeClassifier() # Our model\n",
    "param_grid = {\"max_depth\": np.arange(2,8,1), # Maximum depth of tree\n",
    "              \"max_features\": np.arange(3,8,1), # Number of features to consider when looking for the best split\n",
    "              \"max_leaf_nodes\": np.arange(4,27,1), # Maximum number of leaves in our tree.\n",
    "              \"criterion\": [\"gini\", \"entropy\"], # Splitting criteria\n",
    "              \"class_weight\": [None, 'balanced',{0: 1.105, 1: 1.15}] # Weights associated with classes.\n",
    "            }\n",
    "\n",
    "metric = roc_auc_score # Metric to use\n",
    "tiebreaker = zero_one_loss # Tie breaker metric.\n",
    "n_best_grids = 10 # 5 best grids\n",
    "\n",
    "best_score, best_grid, tiebreaker = gridsearch(model, x_train, y_train, x_val, y_val, param_grid, metric, \n",
    "                                               n_best_grids, loss=False, tiebreaker=tiebreaker)\n",
    "\n",
    "        \n",
    "for a,t,g in zip(best_score, tiebreaker, best_grid):\n",
    "    print(\"AUC:\",a) \n",
    "    print(\"Tie\",t)\n",
    "    print(\"Grid:\",g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(**best_grid[0])\n",
    "model.fit(x_train,y_train)\n",
    "c = confusion_matrix(y_val,model.predict(x_val))\n",
    "sns.heatmap(c, annot=True, xticklabels=xlabel, yticklabels=ylabel) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good result and we could naively choose this single tree, or any of the top 10 combinations as they have the same error, as our model. The problem is that every time we run the parameter search another combination of parameters will be the best tree because of the intrinsic variance of decision trees, additionally these trees will have a really good error as well because of the natural low bias of decision trees. Remember that the trees are tuned to the validation set and might, probably wont, generalize good to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is some pattern to what sort of combinations work for the problem. For the entropy criterion trees the best performing ones the maximum depth is between 5 and 7, the maximum number of leaves between 22 and 26, the maximum number of features to consider on each split around 7 and using balanced weighting for the classes. The balanced weighting adjust the weights for the classes inversely proportional to class frequencies, basically counteracting the false positive hypothesis mentioned earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the gini criterion trees the best performing ones the maximum depth is around 7, the maximum number of leaves varies a lot but tend to approach higher values (25), the maximum number of features to consider on each split around 7 and using the proposed weighting for the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decrease variance and try to make our model to generalize we use need to use an ensemble. The first that comes to mind is trying Bootstrap aggregating or a random forest. It should theoretically provide the stability we need and reduces variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "base = DecisionTreeClassifier(criterion='entropy', splitter='best', \n",
    "                                       max_depth=5, min_samples_split=2, min_samples_leaf=1, \n",
    "                                       min_weight_fraction_leaf=0.0, max_features=7, random_state=None, \n",
    "                                       max_leaf_nodes=22, min_impurity_split=1e-07, \n",
    "                                           class_weight='balanced')\n",
    "\n",
    "model = BaggingClassifier(base_estimator=base, n_estimators=100, max_samples=1.0, max_features=1.0, \n",
    "                  bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, \n",
    "                  n_jobs=1, random_state=None, verbose=0)\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_val)\n",
    "conf = (confusion_matrix(y_val,predictions))\n",
    "auc = (roc_auc_score(y_val,predictions))\n",
    "sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)\n",
    "plt.title(\"AUC  \" + str(auc))\n",
    "print(auc)\n",
    "#plt.savefig('Bagging.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=5, \n",
    "                       min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                       max_features='auto', max_leaf_nodes=22, min_impurity_split=1e-07, \n",
    "                       bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, \n",
    "                       warm_start=False, class_weight='balanced')\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_val)\n",
    "conf = (confusion_matrix(y_val,predictions))\n",
    "auc = (roc_auc_score(y_val,predictions))\n",
    "sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)\n",
    "plt.title(\"AUC  \" + str(auc))\n",
    "print(auc)\n",
    "#plt.savefig('randfor.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensembles seems to provide stability and decrease the variance run to run. However they seem to introduce some bias. Looking back at when we compared entropy and gini criterion and looked at the effect of the depth and variance. Remember that the gini trees generally had few false positives while entropy trees had few false negatives. An viable hypothesis might be that the two complement each other, and because we in the ensembles above only use the one or the other. To test the hypothesis we're gonna try an VotingClassifier using the parameters search result we acquired above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voting classifier consists of a 5:4 ratio of entropy and gini trees since the gini trees showed less potential in the grid search. The classification is done using a majority vote rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "mods = []\n",
    "for i in range(1,100): # 100 Trees provides low variance.\n",
    "    # A parameter combination that were sucessfull for entropy trees.\n",
    "    mods.append((str(i),DecisionTreeClassifier(criterion='entropy', splitter='best', \n",
    "                               max_depth=5, min_samples_split=2, min_samples_leaf=1, \n",
    "                               min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "                               max_leaf_nodes=22, min_impurity_split=1e-07, class_weight='balanced')))\n",
    "    if(i < 80):\n",
    "        # A parameter combination that were sucessfull for gini trees.\n",
    "        mods.append((str(i)+\"gi\",DecisionTreeClassifier(criterion='gini', splitter='best', \n",
    "                               max_depth=7, min_samples_split=2, min_samples_leaf=1, \n",
    "                               min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "                               max_leaf_nodes=25, min_impurity_split=1e-07, \n",
    "                                                        class_weight={0: 1.105, 1: 1.15})))\n",
    "model = VotingClassifier(estimators=mods, voting='hard', n_jobs=1)\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_val)\n",
    "conf = (confusion_matrix(y_val,predictions))\n",
    "auc = (roc_auc_score(y_val,predictions))\n",
    "sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)\n",
    "print(auc)\n",
    "print((roc_auc_score(y_train,model.predict(x_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis might be true, this is a really good separation and a AUC of nearly 89 for the validation set and 93 for the training set. Thats not a gigantic difference and hopefully the constraints on the trees have prevented the model from overfitting on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before giving the test set a go we train our model on the entire training dataset with some imputing. We also put it into a pipeline to automate the work-flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X = pd.read_csv('input/Train.csv')\n",
    "X\n",
    "Y = X['Outcome']\n",
    "X = X.drop([\"Outcome\"], axis=1)\n",
    "\n",
    "model = VotingClassifier(estimators=mods, voting='hard', n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[~X.Pregnancies.isna()]\n",
    "X = X[~X.Glucose.isna()]\n",
    "X = X[~X.BloodPressure.isna()]\n",
    "X = X[~X.SkinThickness.isna()]\n",
    "X = X[~X.Insulin.isna()]\n",
    "X = X[~X.BMI.isna()]\n",
    "X = X[~X.DiabetesPedigreeFunction.isna()]\n",
    "X = X[~X.Age.isna()]\n",
    "X = X[~X.Outcome.isna()]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.dropna()\n",
    "Y=Y.dropna()\n",
    "X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_values='NaN'\n",
    "pipeline = Pipeline([(\"imputer\", SimpleImputer(missing_values = np.nan,\n",
    "                                          strategy=\"mean\"\n",
    "                                          )),\n",
    "                      (\"standardizer\", StandardScaler()),\n",
    "                      (\"VotingClassifier\", model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.fillna(X.mean())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf = dt_clf.fit(X,Y)\n",
    "print(dt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, random_state=None, splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "tree.plot_tree(dt_clf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = tree.export_graphviz(dt_clf, out_file=\"None\", filled=True, rounded=True,special_characters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "from graphviz import Source\n",
    "_data = tree.export_graphviz(dt_clf, out_file=None)\n",
    "graph = graphviz.Source(_data) \n",
    "graph.render(\"name of file\",view = True)\n",
    "import pydotplus\n",
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "_data = tree.export_graphviz(dt_clf, out_file=None, filled=True, rounded=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(_data)  \n",
    "\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "_data = tree.export_graphviz(dt_clf, out_file=None, filled=True, rounded=True,\n",
    "                special_characters=True, max_depth=6)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(_data)  \n",
    "\n",
    "\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate decision tree performance on train and test sets with different tree depths\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# split into train test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# define lists to collect scores\n",
    "train_scores, test_scores = list(), list()\n",
    "# define the tree depths to evaluate\n",
    "values = [i for i in range(1, 21)]\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    "\t# configure the model\n",
    "\tmodel = DecisionTreeClassifier(max_depth=i)\n",
    "\t# fit model on the training dataset\n",
    "\tmodel.fit(X_train, Y_train)\n",
    "\t# evaluate on the train dataset\n",
    "\ttrain_yhat = model.predict(X_train)\n",
    "\ttrain_acc = accuracy_score(Y_train, train_yhat)\n",
    "\ttrain_scores.append(train_acc)\n",
    "\t# evaluate on the test dataset\n",
    "\ttest_yhat = model.predict(X_test)\n",
    "\ttest_acc = accuracy_score(Y_test, test_yhat)\n",
    "\ttest_scores.append(test_acc)\n",
    "\t# summarize progress\n",
    "\tprint('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
    "# plot of train and test scores vs tree depth\n",
    "pyplot.plot(train_scores, values, '-o', label='Train')\n",
    "pyplot.plot(test_scores, values, '-o', label='Test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CART Model\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "DecisionTree = DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_leaf=50)\n",
    "DecisionTree.fit(X,Y)\n",
    "plt.figure(figsize=(20,15))\n",
    "plot_tree(DecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree.fit(X_train, Y_train)\n",
    "Ypred=DecisionTree.predict(X_test)\n",
    "missclf_rate= 1-metrics.accuracy_score(Y_test, Ypred)\n",
    "missclf_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest model\n",
    "ntrees=[]\n",
    "miss_clf_rate=[]\n",
    "for i in range (1,50):\n",
    "    rf=RandomForestClassifier(n_estimators=i)\n",
    "    rf.fit(X_train, Y_train)\n",
    "    Ypred_dt=rf.predict(X_test)\n",
    "    ntrees.append(i)\n",
    "    rate=1-metrics.accuracy_score(Y_test, Ypred)\n",
    "    miss_clf_rate.append(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()#(figsize=(20,10))\n",
    "plt.plot(ntrees, miss_clf_rate, '-o', label='RandomForest')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Missclassification rate')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(X,Y)\n",
    "rf_ypred= rf_clf.predict(X_test)\n",
    "rf_auc=roc_auc_score(Y_test, rf_ypred)\n",
    "rf_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1_clf = DecisionTreeClassifier()\n",
    "dt1_clf=dt1_clf.fit(X_train, Y_train)\n",
    "dt_ypred= dt1_clf.predict(X_test)\n",
    "dt_auc=roc_auc_score(Y_test, dt_ypred)\n",
    "dt_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Misclassification rate plot DecisionTree and RandomForest\n",
    "dt_auc=[]\n",
    "rf_auc=[]\n",
    "rate_dt_auc=[]\n",
    "rate_rf_auc=[]\n",
    "for i in range (1,100):\n",
    "    dt = DecisionTreeClassifier(max_depth=i)\n",
    "    rf = RandomForestClassifier(max_depth=i)\n",
    "    dt.fit(X_train, Y_train)\n",
    "    rf.fit(X_train, Y_train)\n",
    "    \n",
    "    ypred_dt = dt.predict(X_test)\n",
    "    rate_dt=1-metrics.accuracy_score(Y_test, ypred_dt)\n",
    "    rate_dt\n",
    "    auc_dt=roc_auc_score(Y_test, ypred_dt)\n",
    "    dt_auc.append(auc_dt)\n",
    "    rate_dt_auc.append(rate_dt)\n",
    "    \n",
    "    \n",
    "    ypred_rf = rf.predict(X_test)\n",
    "    rate_rf=1-metrics.accuracy_score(Y_test, ypred_rf)\n",
    "    auc_rf=roc_auc_score(Y_test, ypred_rf)\n",
    "    rf_auc.append(auc_rf)\n",
    "    rate_rf_auc.append(rate_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "ax1=fig.add_subplot()\n",
    "#ax1.scatter(range(1,100), dt_auc, s=12, c='g', marker=\"s\", label='Decision Tree')\n",
    "#ax1.scatter(range(1,100), rf_auc, s=12, c='b', marker=\"s\", label='Random Forest')\n",
    "ax1.scatter(range(1,100), rate_dt_auc, s=12, c='g', marker=\"s\", label='Decision Tree')\n",
    "ax1.scatter(range(1,100), rate_rf_auc, s=12, c='b', marker=\"s\", label='Random Forest')\n",
    "plt.title('Misclassication error rate vs. number of trees')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Misclassication error rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn performance on train and test sets with different numbers of neighbors\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib import pyplot\n",
    "# create dataset\n",
    "\n",
    "# split into train test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# define lists to collect scores\n",
    "train_scores, test_scores = list(), list()\n",
    "# define the tree depths to evaluate\n",
    "values = [i for i in range(1, 100)]\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    "\t# configure the model\n",
    "\tmodel = DecisionTreeClassifier(max_depth=i)\n",
    "\t# fit model on the training dataset\n",
    "\tmodel.fit(X_train, Y_train)\n",
    "\t# evaluate on the train dataset\n",
    "\ttrain_yhat = model.predict(X_train)\n",
    "\ttrain_acc = accuracy_score(Y_train, train_yhat)\n",
    "\ttrain_scores.append(train_acc)\n",
    "\t# evaluate on the test dataset\n",
    "\ttest_yhat = model.predict(X_test)\n",
    "\ttest_acc = accuracy_score(Y_test, test_yhat)\n",
    "\ttest_scores.append(test_acc)\n",
    "\t# summarize progress\n",
    "\tprint('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
    "# plot of train and test scores vs number of neighbors\n",
    "pyplot.plot(values, train_scores, '-o', label='Train')\n",
    "pyplot.plot(values, test_scores, '-o', label='Test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "trees = [5, 10, 15, 30, 45, 60, 80, 100]\n",
    "\n",
    "for nb_trees in trees:\n",
    "    rf = RandomForestClassifier(n_estimators=nb_trees)\n",
    "    rf.fit(X_train, Y_train)\n",
    "\n",
    "    train_results.append(mean_squared_error(Y_train, rf.predict(X_train)))\n",
    "    test_results.append(mean_squared_error(Y_test, rf.predict(X_test)))\n",
    "\n",
    "line1, = plt.plot(trees, train_results, color=\"r\", label=\"Training Score\")\n",
    "line2, = plt.plot(trees, test_results, color=\"g\", label=\"Testing Score\")\n",
    "\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('list_nb_trees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Model\n",
    "# fit the model\n",
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.font_manager\n",
    "from sklearn import svm\n",
    "rate_svm=[]\n",
    "rate_rf_auc=[]\n",
    "for i in range (1,100):\n",
    "    clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "    clf.fit(X_train,Y_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    #y_pred_outliers = clf.predict(X_outliers)\n",
    "    n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "    n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "    SVM_rate = 1-metrics.accuracy_score(Y_test, y_pred_test)\n",
    "    SVM_rate\n",
    "    rate_svm.append(SVM_rate)\n",
    "  #  pyplot.plot(rate_svm, gamma, '-o', label='Train')\n",
    "#n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('input/Test.csv')\n",
    "test.dropna(inplace = True)\n",
    "truth = test['Outcome']\n",
    "test.drop('Outcome', axis = 1, inplace = True)\n",
    "predictions = pipeline.predict(test)\n",
    "conf = (confusion_matrix(truth,predictions))\n",
    "acc = (accuracy_score(truth,predictions))\n",
    "auc = (roc_auc_score(truth,predictions))\n",
    "sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)  \n",
    "print(\"accuracy\", acc)\n",
    "print(\"AUC\", auc)\n",
    "#plt.savefig('test_conf.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eureka, this is really good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('input/Test.csv')\n",
    "truth = test['Outcome']\n",
    "test.drop('Outcome', axis = 1, inplace = True)\n",
    "predictions = pipeline.predict(test)\n",
    "conf = (confusion_matrix(truth,predictions))\n",
    "acc = (accuracy_score(truth,predictions))\n",
    "auc = (roc_auc_score(truth,predictions))\n",
    "sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)  \n",
    "print(\"accuracy\", acc)\n",
    "print(\"AUC\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good, but still good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
